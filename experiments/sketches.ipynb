{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 10 videos of cube in bottom right corner and 10 videos of cube in bottom left corner\n",
    "# for each video, save the first 30 frames as images\n",
    "# for each image, extract features using the model\n",
    "# for each feature, train a linear classifier (since we want to see if the features themselves contain info about the corner) to predict the corner of the cube\n",
    "# assess the accuracy of the classifier on the test set and sort the features by their accuracy\n",
    "\n",
    "# also idea - analyze success\n",
    "# after the backbone finished how the net predict where to shift the arm\n",
    "# can plsy whirh dufferebt values of the arm state given the arm position and see where i teasd to\n",
    "\n",
    "# think about mechaitsic way to distinguish between data memoriation and generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting all the features for each image\n",
    "\n",
    "n_features = len(best_features)  # Define the number of features to display\n",
    "features_per_image = []  # List to hold feature maps for each image\n",
    "\n",
    "# Collect all specified features for each initial position\n",
    "for feature_map, img in features_map_of_5_initial_pos:\n",
    "    feature_maps_for_image = [(feature_map[idx], img) for idx in best_features]\n",
    "    features_per_image.append(feature_maps_for_image)\n",
    "\n",
    "# Create a grid with `n_features + 1` rows (1 row for images, and `n_features` for feature maps)\n",
    "fig, axs = plt.subplots(n_features + 1, 5, figsize=(20, (n_features + 1) * 4))\n",
    "\n",
    "# Plot images in the first row  \n",
    "\n",
    "for i, img in enumerate(features_map_of_5_initial_pos):\n",
    "    axs[0, i].imshow(img[1].squeeze(0).permute(1, 2, 0).numpy())  # Use the image from the tuple\n",
    "    axs[0, i].set_title(f'Image {i}')\n",
    "    axs[0, i].axis('off')\n",
    "\n",
    "# Plot each feature map in subsequent rows\n",
    "for row_idx, feature_idx in enumerate(best_features, start=1):\n",
    "    for col_idx, feature_data in enumerate(features_per_image):\n",
    "        feature_map = feature_data[row_idx - 1][0]  # Access the feature map for this row\n",
    "        axs[row_idx, col_idx].imshow(feature_map.detach().numpy(), cmap='gray')\n",
    "        axs[row_idx, col_idx].set_title(f'Feature {feature_idx}')\n",
    "        axs[row_idx, col_idx].axis('off')\n",
    "\n",
    "# Add description text in the figure\n",
    "fig.text(0.5, 0.8, 'ResNet backbone output [15 x 20 x 512] Tensor. Displaying selected features for each initial position (i.e. Tensor[:,:, feature_idx] ).', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting images as grid\n",
    "\n",
    "\n",
    "# for vid_index in img_and_features.keys():\n",
    "#     img, features = img_and_features[vid_index]\n",
    "#     plt.imshow(img.squeeze().permute(1, 2, 0))\n",
    "#     plt.show()\n",
    "#     features = features.squeeze(0).unsqueeze(1).detach()  # Now shape is (512, 15, 20)\n",
    "#     grid = utils.make_grid(features, nrow=32, normalize=False, padding=2)\n",
    "#     plt.figure(figsize=(20, 20))\n",
    "#     plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "\n",
    "# input images collage\n",
    "\n",
    "imgs = []\n",
    "for vid_index in img_and_features.keys():\n",
    "    img, features = img_and_features[vid_index]\n",
    "    imgs.append(img)\n",
    "\n",
    "imgs_batch = torch.cat(imgs, dim=0)  # Remove extra channel dimension\n",
    "\n",
    "# Create the grid\n",
    "grid = utils.make_grid(imgs_batch, nrow=3, normalize=True, padding=2)\n",
    "\n",
    "# Plotting the grid\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# test_feature = 1\n",
    "# test_feature_imgs = []\n",
    "# for vid_index in img_and_features.keys():\n",
    "#     img, features = img_and_features[vid_index]\n",
    "#     test_feature_img = features[0,test_feature,:,:].detach()\n",
    "#     test_feature_imgs.append(test_feature_img)\n",
    "\n",
    "# # plot the images of the test feature on a grid\n",
    "# test_feature_imgs = torch.stack(test_feature_imgs).unsqueeze(1)\n",
    "# grid = utils.make_grid(test_feature_imgs, nrow=3, normalize=False, padding=2)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.imshow(grid.permute(1, 2, 0).cpu().numpy(), cmap='gray')  # use cmap='gray' for grayscale images\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video of grid of all features\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to create and save the video\n",
    "def create_feature_video(img_and_features, output_file='features_video.mp4'):\n",
    "    video_frames = []\n",
    "\n",
    "    # Iterate through each feature (0 to 511)\n",
    "    for feature_idx in range(512):\n",
    "        current_imgs = []\n",
    "        for vid_index in img_and_features.keys():\n",
    "            img, features = img_and_features[vid_index]\n",
    "            feature_img = features[0, feature_idx, :, :].detach()\n",
    "            current_imgs.append(feature_img)\n",
    "\n",
    "        # Stack images to create a grid for the current feature index\n",
    "        stacked_imgs = torch.stack(current_imgs).unsqueeze(1)\n",
    "        grid = utils.make_grid(stacked_imgs, nrow=3, normalize=False, padding=2)\n",
    "        frame = grid.permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        # Convert to uint8 for OpenCV (normalize to range 0-255)\n",
    "        frame = (frame - frame.min()) / (frame.max() - frame.min()) * 255\n",
    "        frame = frame.astype(np.uint8)\n",
    "\n",
    "        # Ensure the frame is C-contiguous for OpenCV compatibility\n",
    "        frame = np.ascontiguousarray(frame)\n",
    "\n",
    "        # Ensure the frame is 3 channels (RGB)\n",
    "        if frame.ndim == 2 or frame.shape[2] == 1:\n",
    "            frame = np.stack([frame] * 3, axis=-1)  # Repeat the channel to create RGB\n",
    "\n",
    "        # Add title (feature number)\n",
    "        # frame = cv2.putText(frame, f'Feature: {feature_idx}', (10, 50),\n",
    "        #                     cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        video_frames.append(frame)\n",
    "\n",
    "    # Set video writer with size of frame\n",
    "    height, width, _ = video_frames[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_file, fourcc, 10, (width, height))  # 10 frames per second\n",
    "\n",
    "    # Write each frame to the video\n",
    "    for frame in video_frames:\n",
    "        out.write(frame)\n",
    "\n",
    "    # Release video writer\n",
    "    out.release()\n",
    "    print(f\"Video saved as {output_file}\")\n",
    "\n",
    "# Call the function to create the video\n",
    "create_feature_video(img_and_features, output_file='features_video.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_features = policy.model.backbone(batch[\"observation.images\"][:, cam_index])[\"feature_map\"]\n",
    "cam_features = cam_features.squeeze(0).detach()  # Now shape is (512, 15, 20)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "observation = {\n",
    "    \"observation.state\": torch.randn((6,)).unsqueeze(0).to(device),  # Example robot state with 6 elements\n",
    "    \"observation.images.laptop\": torch.rand((3, 480, 640), dtype=torch.float32).unsqueeze(0).to(device),  # Random example image\n",
    "    \"observation.images.phone\": torch.rand((3, 480, 640), dtype=torch.float32).unsqueeze(0).to(device),  # Random example image\n",
    "    # \"observation.images.laptop\": laptop_1st_frame.to(device),\n",
    "    # \"observation.images.phone\": phone_1st_frame.to(device),\n",
    "}\n",
    "\n",
    "policy.to(device)\n",
    "policy.eval()  \n",
    "\n",
    "with torch.no_grad():\n",
    "    action = policy.select_action(observation) \n",
    "\n",
    "print(action)\n",
    "\n",
    "batch = policy.normalize_inputs(observation)\n",
    "batch = dict(batch)  # shallow copy so that adding a key doesn't modify the original\n",
    "batch[\"observation.images\"] = torch.stack([batch[k] for k in policy.expected_image_keys], dim=-4)\n",
    "\n",
    "\n",
    "img = batch[\"observation.images.laptop\"]\n",
    "\n",
    "\n",
    "# show the image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img.squeeze().permute(1, 2, 0))\n",
    "plt.show()\n",
    "\n",
    "for cam_index in range(batch[\"observation.images\"].shape[-4]):\n",
    "    img = batch[\"observation.images\"][:, cam_index]\n",
    "    plt.imshow(img.squeeze().permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    cam_features = policy.model.backbone(batch[\"observation.images\"][:, cam_index])[\"feature_map\"]\n",
    "    cam_features = cam_features.squeeze(0).detach()  # Now shape is (512, 15, 20)\n",
    "\n",
    "    # Create a grid of the feature maps (e.g., 32x16 to fit 512 maps)\n",
    "    grid = utils.make_grid(cam_features.unsqueeze(1), nrow=32, normalize=False, padding=2)\n",
    "\n",
    "    # Plot the grid\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# plot grif of features\n",
    "\n",
    "# batch[\"observation.images\"].shape == torch.Size([1, 2, 3, 480, 640])\n",
    "\n",
    "# batch[\"observation.images\"][:, cam_index].shape == torch.Size([1, 3, 480, 640])\n",
    "\n",
    "# cam_features.shape == torch.Size([1, 512, 15, 20])\n",
    "\n",
    "# squeeze the batch dimension and permute the dimensions to match the image dimensions\n",
    "\n",
    "cam_features = cam_features.squeeze(0).permute(1, 2, 0).detach() # shape: (15, 20, 512) # 512 features maps of size 15x20\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils as utils\n",
    "\n",
    "# Assuming `cam_features` is your tensor with shape (15, 20, 512)\n",
    "\n",
    "# Move the feature maps dimension to the first position for plotting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "from lerobot.common.datasets.utils import load_hf_dataset\n",
    "CODEBASE_VERSION = \"v1.6\"\n",
    "split = 'train'\n",
    "root = None\n",
    "repo_id = \"yanivmel1/new_dataset_cube\"\n",
    "hf_dataset = load_hf_dataset(repo_id, CODEBASE_VERSION, root, split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full training loop more need to be imported TBD \n",
    "from lerobot.common.datasets.utils import cycle\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    num_workers=4,\n",
    "    batch_size=1,\n",
    "    shuffle=False, # for training set it to True\n",
    "    sampler=None,\n",
    "    pin_memory=device.type != \"cpu\",\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "dl_iter = cycle(dataloader)\n",
    "for _ in range(1):\n",
    "    batch = next(dl_iter)\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key].to(device, non_blocking=True)\n",
    "\n",
    "offline_steps = 80000\n",
    "step = 0\n",
    "policy.train()\n",
    "for _ in range(step, offline_steps):\n",
    "    start_time = time.perf_counter()\n",
    "    batch = next(dl_iter)\n",
    "    dataloading_s = time.perf_counter() - start_time\n",
    "    for key in batch:\n",
    "        batch[key] = batch[key].to(device, non_blocking=True)\n",
    "    train_info = update_policy(\n",
    "        policy,\n",
    "        batch,\n",
    "        optimizer,\n",
    "        cfg.training.grad_clip_norm,\n",
    "        grad_scaler=grad_scaler,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        use_amp=cfg.use_amp,\n",
    "    )\n",
    "    train_info[\"dataloading_s\"] = dataloading_s\n",
    "    evaluate_and_checkpoint_if_needed(step + 1, is_online=False)\n",
    "    step += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_pretrained_model, get_dataset, set_trainable_layers\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "\n",
    "class ACTPolicyWithIntermediate(nn.Module):\n",
    "    \"\"\"\n",
    "    A wrapper around your ACTPolicy that returns the decoder embeddings\n",
    "    (the final hidden states) *before* the final action_head.\n",
    "    \"\"\"\n",
    "    def __init__(self, original_policy):\n",
    "        super().__init__()\n",
    "        self.original_policy = original_policy\n",
    "        \n",
    "    def forward(self, batch: dict[str, torch.Tensor]) -> torch.Tensor:\n",
    "        batch = self.original_policy.normalize_inputs(batch)\n",
    "        if len(self.original_policy.expected_image_keys) > 0:\n",
    "            batch = dict(batch)\n",
    "            batch[\"observation.images\"] = torch.stack(\n",
    "                [batch[k] for k in self.original_policy.expected_image_keys],\n",
    "                dim=-4\n",
    "            )\n",
    "        batch = self.original_policy.normalize_targets(batch)\n",
    "        \n",
    "        encoder_out, encoder_in_pos_embed, decoder_pos_embed = self.original_policy.model.forward_for_extracting_intermediate(batch)\n",
    "        \n",
    "        return encoder_out, encoder_in_pos_embed, decoder_pos_embed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e418cbcfa1fd4661b440554d830d88a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 46 files:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/295 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_cam_pos_embeds shape torch.Size([1, 512, 15, 40])\n",
      "encoder_in_pos_embed list  602\n",
      "encoder_in_pos_embed - before torch.Size([1, 512])\n",
      "encoder_in_pos_embed - after torch.Size([602, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/295 [21:56<107:31:06, 1316.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_cam_pos_embeds shape torch.Size([1, 512, 15, 40])\n",
      "encoder_in_pos_embed list  602\n",
      "encoder_in_pos_embed - before torch.Size([1, 512])\n",
      "encoder_in_pos_embed - after torch.Size([602, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/295 [22:07<44:38:32, 548.51s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_cam_pos_embeds shape torch.Size([1, 512, 15, 40])\n",
      "encoder_in_pos_embed list  602\n",
      "encoder_in_pos_embed - before torch.Size([1, 512])\n",
      "encoder_in_pos_embed - after torch.Size([602, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/295 [22:18<54:29:09, 669.45s/it]\n"
     ]
    }
   ],
   "source": [
    "policy_repo_id = \"yanivmel1/new_dataset_cube_080000\"\n",
    "policy = get_pretrained_model(policy_repo_id)\n",
    "# policy = set_trainable_layers(policy)\n",
    "\n",
    "# Wrap it\n",
    "policy_for_embeddings = ACTPolicyWithIntermediate(policy)\n",
    "policy_for_embeddings.eval()\n",
    "\n",
    "# Load your dataset\n",
    "training_data_repo_id = \"yanivmel1/new_dataset_cube\"\n",
    "val_set_repo_id = \"yanivmel1/fine_tune_1\"\n",
    "dataset = get_dataset(val_set_repo_id)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Generate embeddings\n",
    "all_encoder_out = []\n",
    "all_encoder_in_pos_embed = []\n",
    "all_decoder_pos_embed = []\n",
    "all_labels = []\n",
    "policy_for_embeddings.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    ## add tqdm for progress bar\n",
    "    i = 0 \n",
    "    for batch in tqdm.tqdm(data_loader):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(device)\n",
    "        # This returns [B, chunk_size, dim_model]\n",
    "        encoder_out, encoder_in_pos_embed, decoder_pos_embed = policy_for_embeddings(batch)\n",
    "        # Save them\n",
    "        all_encoder_out.append(encoder_out.cpu())\n",
    "        all_encoder_in_pos_embed.append(encoder_in_pos_embed.cpu())\n",
    "        all_decoder_pos_embed.append(decoder_pos_embed.cpu())\n",
    "        all_labels.append(batch[\"action\"].cpu())  # or whatever label you need\n",
    "        i += 1\n",
    "        if i == 3:\n",
    "            break\n",
    "\n",
    "# Done: cat & save them as your new dataset\n",
    "# check if all elements in all_encoder_in_pos_embed are identical\n",
    "# x = all_encoder_in_pos_embed[0]\n",
    "# for i in range(1, len(all_encoder_in_pos_embed)):\n",
    "#     if not torch.equal(x, all_encoder_in_pos_embed[i]):\n",
    "#         print(\"not equal - encoder_in_pos_embed\")\n",
    "# y = all_decoder_pos_embed[0]\n",
    "# for i in range(1, len(all_decoder_pos_embed)):\n",
    "#     if not torch.equal(y, all_decoder_pos_embed[i]):\n",
    "#         print(\"not equal - decoder\")\n",
    "\n",
    "# all_encoder_out = torch.cat(all_encoder_out, dim=0)  # shape [N, chunk_size, dim_model]\n",
    "# all_encoder_in_pos_embed = torch.cat(all_encoder_in_pos_embed, dim=0)  # shape [N, chunk_size, dim_model]\n",
    "# all_decoder_pos_embed = torch.cat(all_decoder_pos_embed, dim=0)  # shape [N, chunk_size, dim_model]\n",
    "# all_labels = torch.cat(all_labels, dim=0)          # shape [N, chunk_size, action_dim]?\n",
    "# torch.save({\"encoder_out\": all_encoder_out, \"encoder_in_pos_embed\": all_encoder_in_pos_embed, \"decoder_pos_embed\": all_decoder_pos_embed, \"labels\": all_labels}, \"intermediate_dataset.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([602, 1, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(all_encoder_in_pos_embed) == 3\n",
    "# (all_encoder_in_pos_embed[2] == all_encoder_in_pos_embed[1]).all()\n",
    "all_encoder_in_pos_embed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Embedding' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# len(all_encoder_out) == 3\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mall_decoder_pos_embed\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Embedding' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# len(all_encoder_out) == 3\n",
    "all_decoder_pos_embed[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_out: torch.Size([602, 9421, 512])\n",
      "encoder_in_pos_embed: torch.Size([602, 295, 512])\n",
      "decoder_pos_embed: torch.Size([295, 100, 512])\n",
      "labels: torch.Size([9421, 100, 6])\n",
      "action_is_pad: torch.Size([9421, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data = torch.load(\"val_embeddings.pt\")\n",
    "\n",
    "# Print the dimensions of each tensor\n",
    "for key, value in data.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: Not a tensor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"action_is_pad\"]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not all matrices are identical.\n"
     ]
    }
   ],
   "source": [
    "test_tensor = data[\"decoder_pos_embed\"]\n",
    "first_matrix = test_tensor[0, :, :]\n",
    "\n",
    "# Check if all matrices along dim 1 are equal to the first matrix\n",
    "are_identical = torch.all(test_tensor == first_matrix.unsqueeze(1))\n",
    "\n",
    "if are_identical:\n",
    "    print(\"All matrices are identical.\")\n",
    "else:\n",
    "    print(\"Not all matrices are identical.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n",
      "normalize_inputs.buffer_observation_images_laptop.mean: requires_grad=False\n",
      "normalize_inputs.buffer_observation_images_laptop.std: requires_grad=False\n",
      "normalize_inputs.buffer_observation_images_phone.mean: requires_grad=False\n",
      "normalize_inputs.buffer_observation_images_phone.std: requires_grad=False\n",
      "normalize_inputs.buffer_observation_state.mean: requires_grad=False\n",
      "normalize_inputs.buffer_observation_state.std: requires_grad=False\n",
      "normalize_targets.buffer_action.mean: requires_grad=False\n",
      "normalize_targets.buffer_action.std: requires_grad=False\n",
      "unnormalize_outputs.buffer_action.mean: requires_grad=False\n",
      "unnormalize_outputs.buffer_action.std: requires_grad=False\n",
      "model.vae_encoder.layers.0.self_attn.in_proj_weight: requires_grad=False\n",
      "model.vae_encoder.layers.0.self_attn.in_proj_bias: requires_grad=False\n",
      "model.vae_encoder.layers.0.self_attn.out_proj.weight: requires_grad=False\n",
      "model.vae_encoder.layers.0.self_attn.out_proj.bias: requires_grad=False\n",
      "model.vae_encoder.layers.0.linear1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.0.linear1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.0.linear2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.0.linear2.bias: requires_grad=False\n",
      "model.vae_encoder.layers.0.norm1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.0.norm1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.0.norm2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.0.norm2.bias: requires_grad=False\n",
      "model.vae_encoder.layers.1.self_attn.in_proj_weight: requires_grad=False\n",
      "model.vae_encoder.layers.1.self_attn.in_proj_bias: requires_grad=False\n",
      "model.vae_encoder.layers.1.self_attn.out_proj.weight: requires_grad=False\n",
      "model.vae_encoder.layers.1.self_attn.out_proj.bias: requires_grad=False\n",
      "model.vae_encoder.layers.1.linear1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.1.linear1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.1.linear2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.1.linear2.bias: requires_grad=False\n",
      "model.vae_encoder.layers.1.norm1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.1.norm1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.1.norm2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.1.norm2.bias: requires_grad=False\n",
      "model.vae_encoder.layers.2.self_attn.in_proj_weight: requires_grad=False\n",
      "model.vae_encoder.layers.2.self_attn.in_proj_bias: requires_grad=False\n",
      "model.vae_encoder.layers.2.self_attn.out_proj.weight: requires_grad=False\n",
      "model.vae_encoder.layers.2.self_attn.out_proj.bias: requires_grad=False\n",
      "model.vae_encoder.layers.2.linear1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.2.linear1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.2.linear2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.2.linear2.bias: requires_grad=False\n",
      "model.vae_encoder.layers.2.norm1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.2.norm1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.2.norm2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.2.norm2.bias: requires_grad=False\n",
      "model.vae_encoder.layers.3.self_attn.in_proj_weight: requires_grad=False\n",
      "model.vae_encoder.layers.3.self_attn.in_proj_bias: requires_grad=False\n",
      "model.vae_encoder.layers.3.self_attn.out_proj.weight: requires_grad=False\n",
      "model.vae_encoder.layers.3.self_attn.out_proj.bias: requires_grad=False\n",
      "model.vae_encoder.layers.3.linear1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.3.linear1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.3.linear2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.3.linear2.bias: requires_grad=False\n",
      "model.vae_encoder.layers.3.norm1.weight: requires_grad=False\n",
      "model.vae_encoder.layers.3.norm1.bias: requires_grad=False\n",
      "model.vae_encoder.layers.3.norm2.weight: requires_grad=False\n",
      "model.vae_encoder.layers.3.norm2.bias: requires_grad=False\n",
      "model.vae_encoder_cls_embed.weight: requires_grad=False\n",
      "model.vae_encoder_robot_state_input_proj.weight: requires_grad=False\n",
      "model.vae_encoder_robot_state_input_proj.bias: requires_grad=False\n",
      "model.vae_encoder_action_input_proj.weight: requires_grad=False\n",
      "model.vae_encoder_action_input_proj.bias: requires_grad=False\n",
      "model.vae_encoder_latent_output_proj.weight: requires_grad=False\n",
      "model.vae_encoder_latent_output_proj.bias: requires_grad=False\n",
      "model.backbone.conv1.weight: requires_grad=False\n",
      "model.backbone.layer1.0.conv1.weight: requires_grad=False\n",
      "model.backbone.layer1.0.conv2.weight: requires_grad=False\n",
      "model.backbone.layer1.1.conv1.weight: requires_grad=False\n",
      "model.backbone.layer1.1.conv2.weight: requires_grad=False\n",
      "model.backbone.layer2.0.conv1.weight: requires_grad=False\n",
      "model.backbone.layer2.0.conv2.weight: requires_grad=False\n",
      "model.backbone.layer2.0.downsample.0.weight: requires_grad=False\n",
      "model.backbone.layer2.1.conv1.weight: requires_grad=False\n",
      "model.backbone.layer2.1.conv2.weight: requires_grad=False\n",
      "model.backbone.layer3.0.conv1.weight: requires_grad=False\n",
      "model.backbone.layer3.0.conv2.weight: requires_grad=False\n",
      "model.backbone.layer3.0.downsample.0.weight: requires_grad=False\n",
      "model.backbone.layer3.1.conv1.weight: requires_grad=False\n",
      "model.backbone.layer3.1.conv2.weight: requires_grad=False\n",
      "model.backbone.layer4.0.conv1.weight: requires_grad=False\n",
      "model.backbone.layer4.0.conv2.weight: requires_grad=False\n",
      "model.backbone.layer4.0.downsample.0.weight: requires_grad=False\n",
      "model.backbone.layer4.1.conv1.weight: requires_grad=False\n",
      "model.backbone.layer4.1.conv2.weight: requires_grad=False\n",
      "model.encoder.layers.0.self_attn.in_proj_weight: requires_grad=False\n",
      "model.encoder.layers.0.self_attn.in_proj_bias: requires_grad=False\n",
      "model.encoder.layers.0.self_attn.out_proj.weight: requires_grad=False\n",
      "model.encoder.layers.0.self_attn.out_proj.bias: requires_grad=False\n",
      "model.encoder.layers.0.linear1.weight: requires_grad=False\n",
      "model.encoder.layers.0.linear1.bias: requires_grad=False\n",
      "model.encoder.layers.0.linear2.weight: requires_grad=False\n",
      "model.encoder.layers.0.linear2.bias: requires_grad=False\n",
      "model.encoder.layers.0.norm1.weight: requires_grad=False\n",
      "model.encoder.layers.0.norm1.bias: requires_grad=False\n",
      "model.encoder.layers.0.norm2.weight: requires_grad=False\n",
      "model.encoder.layers.0.norm2.bias: requires_grad=False\n",
      "model.encoder.layers.1.self_attn.in_proj_weight: requires_grad=False\n",
      "model.encoder.layers.1.self_attn.in_proj_bias: requires_grad=False\n",
      "model.encoder.layers.1.self_attn.out_proj.weight: requires_grad=False\n",
      "model.encoder.layers.1.self_attn.out_proj.bias: requires_grad=False\n",
      "model.encoder.layers.1.linear1.weight: requires_grad=False\n",
      "model.encoder.layers.1.linear1.bias: requires_grad=False\n",
      "model.encoder.layers.1.linear2.weight: requires_grad=False\n",
      "model.encoder.layers.1.linear2.bias: requires_grad=False\n",
      "model.encoder.layers.1.norm1.weight: requires_grad=False\n",
      "model.encoder.layers.1.norm1.bias: requires_grad=False\n",
      "model.encoder.layers.1.norm2.weight: requires_grad=False\n",
      "model.encoder.layers.1.norm2.bias: requires_grad=False\n",
      "model.encoder.layers.2.self_attn.in_proj_weight: requires_grad=False\n",
      "model.encoder.layers.2.self_attn.in_proj_bias: requires_grad=False\n",
      "model.encoder.layers.2.self_attn.out_proj.weight: requires_grad=False\n",
      "model.encoder.layers.2.self_attn.out_proj.bias: requires_grad=False\n",
      "model.encoder.layers.2.linear1.weight: requires_grad=False\n",
      "model.encoder.layers.2.linear1.bias: requires_grad=False\n",
      "model.encoder.layers.2.linear2.weight: requires_grad=False\n",
      "model.encoder.layers.2.linear2.bias: requires_grad=False\n",
      "model.encoder.layers.2.norm1.weight: requires_grad=False\n",
      "model.encoder.layers.2.norm1.bias: requires_grad=False\n",
      "model.encoder.layers.2.norm2.weight: requires_grad=False\n",
      "model.encoder.layers.2.norm2.bias: requires_grad=False\n",
      "model.encoder.layers.3.self_attn.in_proj_weight: requires_grad=False\n",
      "model.encoder.layers.3.self_attn.in_proj_bias: requires_grad=False\n",
      "model.encoder.layers.3.self_attn.out_proj.weight: requires_grad=False\n",
      "model.encoder.layers.3.self_attn.out_proj.bias: requires_grad=False\n",
      "model.encoder.layers.3.linear1.weight: requires_grad=False\n",
      "model.encoder.layers.3.linear1.bias: requires_grad=False\n",
      "model.encoder.layers.3.linear2.weight: requires_grad=False\n",
      "model.encoder.layers.3.linear2.bias: requires_grad=False\n",
      "model.encoder.layers.3.norm1.weight: requires_grad=False\n",
      "model.encoder.layers.3.norm1.bias: requires_grad=False\n",
      "model.encoder.layers.3.norm2.weight: requires_grad=False\n",
      "model.encoder.layers.3.norm2.bias: requires_grad=False\n",
      "model.decoder.layers.0.self_attn.in_proj_weight: requires_grad=False\n",
      "model.decoder.layers.0.self_attn.in_proj_bias: requires_grad=False\n",
      "model.decoder.layers.0.self_attn.out_proj.weight: requires_grad=False\n",
      "model.decoder.layers.0.self_attn.out_proj.bias: requires_grad=False\n",
      "model.decoder.layers.0.multihead_attn.in_proj_weight: requires_grad=False\n",
      "model.decoder.layers.0.multihead_attn.in_proj_bias: requires_grad=False\n",
      "model.decoder.layers.0.multihead_attn.out_proj.weight: requires_grad=False\n",
      "model.decoder.layers.0.multihead_attn.out_proj.bias: requires_grad=False\n",
      "model.decoder.layers.0.linear1.weight: requires_grad=True\n",
      "model.decoder.layers.0.linear1.bias: requires_grad=True\n",
      "model.decoder.layers.0.linear2.weight: requires_grad=True\n",
      "model.decoder.layers.0.linear2.bias: requires_grad=True\n",
      "model.decoder.layers.0.norm1.weight: requires_grad=False\n",
      "model.decoder.layers.0.norm1.bias: requires_grad=False\n",
      "model.decoder.layers.0.norm2.weight: requires_grad=False\n",
      "model.decoder.layers.0.norm2.bias: requires_grad=False\n",
      "model.decoder.layers.0.norm3.weight: requires_grad=True\n",
      "model.decoder.layers.0.norm3.bias: requires_grad=True\n",
      "model.decoder.norm.weight: requires_grad=False\n",
      "model.decoder.norm.bias: requires_grad=False\n",
      "model.encoder_robot_state_input_proj.weight: requires_grad=False\n",
      "model.encoder_robot_state_input_proj.bias: requires_grad=False\n",
      "model.encoder_latent_input_proj.weight: requires_grad=False\n",
      "model.encoder_latent_input_proj.bias: requires_grad=False\n",
      "model.encoder_img_feat_input_proj.weight: requires_grad=False\n",
      "model.encoder_img_feat_input_proj.bias: requires_grad=False\n",
      "model.encoder_1d_feature_pos_embed.weight: requires_grad=False\n",
      "model.decoder_pos_embed.weight: requires_grad=False\n",
      "model.action_head.weight: requires_grad=True\n",
      "model.action_head.bias: requires_grad=True\n",
      "Number of learnable parameters: 3284614\n",
      "Number of total parameters: 51597238\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e75fcf4fca24690b264081c1f5cd123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 106 files:   0%|          | 0/106 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f83eae8f84f420cb3aeac0ba2462e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 46 files:   0%|          | 0/46 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [01:19<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 265\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(step, offline_steps)):\n\u001b[0;32m    264\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m--> 265\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdl_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m     dataloading_s \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m batch:\n",
      "File \u001b[1;32m~\\OneDrive - Microsoft\\Desktop\\personal\\Robot\\new_robot\\lerobot\\lerobot\\common\\datasets\\utils.py:395\u001b[0m, in \u001b[0;36mcycle\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcycle\u001b[39m(iterable):\n\u001b[0;32m    391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The equivalent of itertools.cycle, but safe for Pytorch dataloaders.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \n\u001b[0;32m    393\u001b[0m \u001b[38;5;124;03m    See https://github.com/pytorch/pytorch/issues/23900 for information on why itertools.cycle is not safe.\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 395\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\t-ymelamed\\AppData\\Local\\miniconda3\\envs\\lerobot\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
